{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\anaconda\\envs\\torch\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from einops import rearrange #pip install einops\n",
    "from typing import List\n",
    "import random\n",
    "import math\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from timm.utils import ModelEmaV3 \n",
    "from tqdm import tqdm \n",
    "import matplotlib.pyplot as plt \n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import os  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SinusoidalEmbeddings(nn.Module):\n",
    "    \"\"\"\n",
    "    使用正弦/余弦函数对时间步（timestep）进行位置编码，将离散的时间步 t 映射为连续的、具有周期性的高维向量。\n",
    "    \"\"\"\n",
    "    def __init__(self, time_steps: int, embed_dim: int):\n",
    "        super().__init__()\n",
    "        # 创建形状为 [time_steps, 1] 的时间步索引\n",
    "        position = torch.arange(time_steps).unsqueeze(1).float()  \n",
    "\n",
    "        # 计算频率缩放因子，用于控制不同维度的周期\n",
    "        div = torch.exp(\n",
    "            torch.arange(0, embed_dim, 2).float() * -(math.log(10000.0) / embed_dim)\n",
    "        ) \n",
    "\n",
    "        embeddings = torch.zeros(time_steps, embed_dim, requires_grad=False)\n",
    "        embeddings[:, 0::2] = torch.sin(position * div)        # 偶数维度用 sin 编码\n",
    "        embeddings[:, 1::2] = torch.cos(position * div)      # 奇数维度用 cos 编码\n",
    "        self.register_buffer('embeddings', embeddings)\n",
    "\n",
    "    def forward(self, x, t):\n",
    "        \"\"\"\n",
    "        根据输入的时间步索引 t，取出对应的时间嵌入向量，并调整形状以匹配特征图 x。\n",
    "        \"\"\"\n",
    "        # 从预计算的 embeddings 表中取出 t 对应的嵌入向量\n",
    "        embeds = self.embeddings[t].to(x.device)\n",
    "\n",
    "        # 添加两个维度 (H=1, W=1)，变为 (B, embed_dim, 1, 1)\n",
    "        # 这样在后续与特征图 x (B, C, H, W) 相加时，embeds 会自动广播到每个空间位置\n",
    "        return embeds[:, :, None, None]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#残差网络， 带时间嵌入注入的残差块，使网络能根据当前时间步 t 调整去噪行为。\n",
    "class ResBlock(nn.Module):\n",
    "    def __init__(self, C: int, num_groups: int, dropout_prob: float):\n",
    "        super().__init__()\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.gnorm1 = nn.GroupNorm(num_groups=num_groups, num_channels=C)\n",
    "        self.gnorm2 = nn.GroupNorm(num_groups=num_groups, num_channels=C)\n",
    "        self.conv1 = nn.Conv2d(C, C, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(C, C, kernel_size=3, padding=1)\n",
    "        self.dropout = nn.Dropout(p=dropout_prob, inplace=True)\n",
    "\n",
    "    def forward(self, x, embeddings):\n",
    "        x = x + embeddings[:, :x.shape[1], :, :]\n",
    "        r = self.conv1(self.relu(self.gnorm1(x)))\n",
    "        r = self.dropout(r)\n",
    "        r = self.conv2(self.relu(self.gnorm2(r)))\n",
    "        return r + x\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#将输入特征图的每个空间位置视为一个 token，计算它们之间的全局依赖关系。\n",
    "class Attention(nn.Module):\n",
    "    def __init__(self, C: int, num_heads: int, dropout_prob: float):\n",
    "        super().__init__()\n",
    "        self.proj1 = nn.Linear(C, C * 3)\n",
    "        self.proj2 = nn.Linear(C, C)\n",
    "        self.num_heads = num_heads\n",
    "        self.dropout_prob = dropout_prob\n",
    "\n",
    "    def forward(self, x):\n",
    "        h, w = x.shape[2:]\n",
    "        x = rearrange(x, 'b c h w -> b (h w) c')#将空间维度展平为序列 \n",
    "        x = self.proj1(x)#线性投影得到 Q, K, V \n",
    "        head_dim = x.shape[-1] // (3 * self.num_heads)# # 计算每个注意力头的维度\n",
    "        \n",
    "        x = rearrange(x, 'b L (K H C) -> K b H L C', K=3, H=self.num_heads, C=head_dim)\n",
    "        q, k, v = x[0], x[1], x[2]\n",
    "        x = F.scaled_dot_product_attention(q, k, v, is_causal=False, dropout_p=self.dropout_prob)\n",
    "        x = rearrange(x, 'b H L C -> b L (H C)')\n",
    "        x = rearrange(x, 'b (h w) C -> b h w C', h=h, w=w)\n",
    "        x = self.proj2(x)\n",
    "        return rearrange(x, 'b h w C -> b C h w')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DDPM_Scheduler(nn.Module):\n",
    "    def __init__(self, num_time_steps: int = 1000):\n",
    "        super().__init__()\n",
    "        self.register_buffer('beta', torch.linspace(1e-4, 0.02, num_time_steps))\n",
    "        alpha = 1.0 - self.beta\n",
    "        self.register_buffer('alpha', torch.cumprod(alpha, dim=0))\n",
    "\n",
    "    def forward(self, t):\n",
    "        return self.beta[t], self.alpha[t]\n",
    "\n",
    "\n",
    "def set_seed(seed: int = 42):\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "\n",
    "\n",
    "class UnetLayer(nn.Module):\n",
    "    def __init__(self, upscale: bool, attention: bool, num_groups: int, dropout_prob: float, num_heads: int, C: int):\n",
    "        super().__init__()\n",
    "        self.ResBlock1 = ResBlock(C=C, num_groups=num_groups, dropout_prob=dropout_prob)\n",
    "        self.ResBlock2 = ResBlock(C=C, num_groups=num_groups, dropout_prob=dropout_prob)\n",
    "        if upscale:\n",
    "            self.conv = nn.ConvTranspose2d(C, C // 2, kernel_size=4, stride=2, padding=1)\n",
    "        else:\n",
    "            self.conv = nn.Conv2d(C, C * 2, kernel_size=3, stride=2, padding=1)\n",
    "        if attention:\n",
    "            self.attention_layer = Attention(C, num_heads=num_heads, dropout_prob=dropout_prob)\n",
    "\n",
    "    def forward(self, x, embeddings):\n",
    "        x = self.ResBlock1(x, embeddings)\n",
    "        if hasattr(self, 'attention_layer'):\n",
    "            x = self.attention_layer(x)\n",
    "        x = self.ResBlock2(x, embeddings)\n",
    "        return self.conv(x), x\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UNET(nn.Module):\n",
    "    def __init__(self,\n",
    "                 Channels: List = [64, 128, 256, 512, 512, 384],\n",
    "                 Attentions: List = [False, True, False, False, False, True],\n",
    "                 Upscales: List = [False, False, False, True, True, True],\n",
    "                 num_groups: int = 32,\n",
    "                 dropout_prob: float = 0.1,\n",
    "                 num_heads: int = 8,\n",
    "                 input_channels: int = 1,\n",
    "                 output_channels: int = 1,\n",
    "                 time_steps: int = 1000):\n",
    "        super().__init__()\n",
    "        self.num_layers = len(Channels)\n",
    "        self.shallow_conv = nn.Conv2d(input_channels, Channels[0], kernel_size=3, padding=1)\n",
    "        out_channels = (Channels[-1] // 2) + Channels[0]\n",
    "        self.late_conv = nn.Conv2d(out_channels, out_channels // 2, kernel_size=3, padding=1)\n",
    "        self.output_conv = nn.Conv2d(out_channels // 2, output_channels, kernel_size=1)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.embeddings = SinusoidalEmbeddings(time_steps=time_steps, embed_dim=max(Channels))\n",
    "        for i in range(self.num_layers):\n",
    "            layer = UnetLayer(\n",
    "                upscale=Upscales[i],\n",
    "                attention=Attentions[i],\n",
    "                num_groups=num_groups,\n",
    "                dropout_prob=dropout_prob,\n",
    "                C=Channels[i],\n",
    "                num_heads=num_heads\n",
    "            )\n",
    "            setattr(self, f'Layer{i+1}', layer)\n",
    "\n",
    "    def forward(self, x, t):\n",
    "        x = self.shallow_conv(x)\n",
    "        residuals = []\n",
    "        # Encoder\n",
    "        for i in range(self.num_layers // 2):\n",
    "            layer = getattr(self, f'Layer{i+1}')\n",
    "            embeddings = self.embeddings(x, t)\n",
    "            x, r = layer(x, embeddings)\n",
    "            residuals.append(r)\n",
    "        # Decoder\n",
    "        for i in range(self.num_layers // 2, self.num_layers):\n",
    "            layer = getattr(self, f'Layer{i+1}')\n",
    "            embeddings = self.embeddings(x, t)\n",
    "            x, _ = layer(x, embeddings)\n",
    "            x = torch.cat((x, residuals[self.num_layers - i - 1]), dim=1)\n",
    "        x = self.late_conv(x)\n",
    "        x = self.relu(x)\n",
    "        return self.output_conv(x)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ======================\n",
    "# 训练函数（首次训练）\n",
    "# ======================\n",
    "def train(batch_size: int = 64,\n",
    "          num_time_steps: int = 1000,\n",
    "          num_epochs: int = 15,\n",
    "          seed: int = -1,\n",
    "          ema_decay: float = 0.9999,\n",
    "          lr=2e-5,\n",
    "          checkpoint_path: str = None):\n",
    "    set_seed(random.randint(0, 2**32-1)) if seed == -1 else set_seed(seed)\n",
    "\n",
    "    train_dataset = datasets.MNIST(root='./data', train=True, download=True, transform=transforms.ToTensor())\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, drop_last=True, num_workers=2)\n",
    "\n",
    "    scheduler = DDPM_Scheduler(num_time_steps=num_time_steps).cuda()  # ← 关键修改！\n",
    "    model = UNET().cuda()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    ema = ModelEmaV3(model, decay=ema_decay)\n",
    "\n",
    "    # 如果是继续训练，才加载 checkpoint\n",
    "    if checkpoint_path is not None and os.path.exists(checkpoint_path):\n",
    "        print(f\"Loading checkpoint from {checkpoint_path}\")\n",
    "        checkpoint = torch.load(checkpoint_path)\n",
    "        model.load_state_dict(checkpoint['weights'])\n",
    "        ema.load_state_dict(checkpoint['ema'])\n",
    "        optimizer.load_state_dict(checkpoint['optimizer'])\n",
    "\n",
    "    criterion = nn.MSELoss(reduction='mean')\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        total_loss = 0\n",
    "        pbar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs}\")\n",
    "        for x, _ in pbar:\n",
    "            x = x.cuda()\n",
    "            x = F.pad(x, (2, 2, 2, 2))  # 28x28 -> 32x32\n",
    "            t = torch.randint(0, num_time_steps, (x.shape[0],), device=x.device)\n",
    "            e = torch.randn_like(x)\n",
    "            alpha_t = scheduler.alpha[t].view(-1, 1, 1, 1)\n",
    "            x_noisy = torch.sqrt(alpha_t) * x + torch.sqrt(1 - alpha_t) * e\n",
    "\n",
    "            pred_noise = model(x_noisy, t)\n",
    "            loss = criterion(pred_noise, e)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            ema.update(model)\n",
    "\n",
    "            total_loss += loss.item()\n",
    "            pbar.set_postfix({'loss': loss.item()})\n",
    "\n",
    "        avg_loss = total_loss / len(train_loader)\n",
    "        print(f'Epoch {epoch+1} | Avg Loss: {avg_loss:.5f}')\n",
    "\n",
    "    # 保存模型\n",
    "    os.makedirs('checkpoints', exist_ok=True)\n",
    "    checkpoint = {\n",
    "        'weights': model.state_dict(),\n",
    "        'ema': ema.state_dict(),\n",
    "        'optimizer': optimizer.state_dict()\n",
    "    }\n",
    "    save_path = 'checkpoints/ddpm_checkpoint.pth'\n",
    "    torch.save(checkpoint, save_path)\n",
    "    print(f\"Model saved to {save_path}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def display_reverse(images: List):\n",
    "    if len(images) > 10:\n",
    "        images = images[:10]\n",
    "    fig, axes = plt.subplots(1, len(images), figsize=(len(images), 1))\n",
    "    if len(images) == 1:\n",
    "        axes = [axes]\n",
    "    for ax, img in zip(axes, images):\n",
    "        img = img.squeeze(0).cpu().numpy()\n",
    "        if img.shape[0] == 1:\n",
    "            img = img[0]\n",
    "        img = np.clip(img, 0, 1)\n",
    "        ax.imshow(img, cmap='gray')\n",
    "        ax.axis('off')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def inference(checkpoint_path: str = 'checkpoints/ddpm_checkpoint.pth',\n",
    "              num_time_steps: int = 1000,\n",
    "              ema_decay: float = 0.9999):\n",
    "    assert os.path.exists(checkpoint_path), f\"Checkpoint not found: {checkpoint_path}\"\n",
    "    checkpoint = torch.load(checkpoint_path)\n",
    "    model = UNET().cuda()\n",
    "    ema = ModelEmaV3(model, decay=ema_decay)\n",
    "    ema.load_state_dict(checkpoint['ema'])\n",
    "    model = ema.module.eval()\n",
    "\n",
    "    scheduler = DDPM_Scheduler(num_time_steps=num_time_steps).cuda()  # ← 也加 .cuda()\n",
    "    times_to_show = [999, 700, 550, 400, 300, 200, 100, 50, 15, 0]\n",
    "\n",
    "    with torch.no_grad():\n",
    "        x_t = torch.randn(1, 1, 32, 32).cuda()\n",
    "        images = []\n",
    "\n",
    "        for t in reversed(range(num_time_steps)):\n",
    "            t_batch = torch.tensor([t], device=x_t.device)\n",
    "            beta_t = scheduler.beta[t]\n",
    "            alpha_t = scheduler.alpha[t]\n",
    "            alpha_prev = scheduler.alpha[t-1] if t > 0 else torch.tensor(1.0)\n",
    "\n",
    "            pred_noise = model(x_t, t_batch)\n",
    "\n",
    "            # 计算 x_{t-1}\n",
    "            coef1 = 1.0 / torch.sqrt(1.0 - beta_t)\n",
    "            coef2 = beta_t / torch.sqrt(1.0 - alpha_t)\n",
    "            x0_pred = (x_t - torch.sqrt(1.0 - alpha_t) * pred_noise) / torch.sqrt(alpha_t)\n",
    "            mean = coef1 * (x_t - coef2 * pred_noise)\n",
    "\n",
    "            if t > 0:\n",
    "                sigma_t = torch.sqrt(beta_t * (1.0 - alpha_prev) / (1.0 - alpha_t))\n",
    "                noise = torch.randn_like(x_t)\n",
    "                x_t = mean + sigma_t * noise\n",
    "            else:\n",
    "                x_t = mean\n",
    "\n",
    "            if t in times_to_show:\n",
    "                img = x_t.cpu().clone()\n",
    "                images.append(img)\n",
    "\n",
    "        # 显示结果\n",
    "        display_reverse(images)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def main():\n",
    "    # 第一次训练：checkpoint_path=None\n",
    "    train(\n",
    "        batch_size=64,\n",
    "        num_epochs=70,        # 先训 10 轮测试\n",
    "        lr=2e-5,              # 可稍大学习率加速收敛\n",
    "        checkpoint_path=None  # ←←← 关键：首次训练设为 None\n",
    "    )\n",
    "    # 训练完后自动推理\n",
    "    inference('checkpoints/ddpm_checkpoint.pth')\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
