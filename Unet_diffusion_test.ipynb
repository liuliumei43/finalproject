{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\anaconda\\envs\\torch\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from einops import rearrange #pip install einops\n",
    "from typing import List\n",
    "import random\n",
    "import math\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from timm.utils import ModelEmaV3 \n",
    "from tqdm import tqdm \n",
    "import matplotlib.pyplot as plt \n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import os  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SinusoidalEmbeddings(nn.Module):\n",
    "    \"\"\"\n",
    "    使用正弦/余弦函数对时间步（timestep）进行位置编码，将离散的时间步 t 映射为连续的、具有周期性的高维向量。\n",
    "    \"\"\"\n",
    "    def __init__(self, time_steps: int, embed_dim: int):\n",
    "        super().__init__()\n",
    "        # 创建形状为 [time_steps, 1] 的时间步索引\n",
    "        position = torch.arange(time_steps).unsqueeze(1).float()  \n",
    "\n",
    "        # 计算频率缩放因子，用于控制不同维度的周期\n",
    "        div = torch.exp(\n",
    "            torch.arange(0, embed_dim, 2).float() * -(math.log(10000.0) / embed_dim)\n",
    "        ) \n",
    "\n",
    "        embeddings = torch.zeros(time_steps, embed_dim, requires_grad=False)\n",
    "        embeddings[:, 0::2] = torch.sin(position * div)        # 偶数维度用 sin 编码\n",
    "        embeddings[:, 1::2] = torch.cos(position * div)      # 奇数维度用 cos 编码\n",
    "        self.register_buffer('embeddings', embeddings)\n",
    "\n",
    "    def forward(self, x, t):\n",
    "        \"\"\"\n",
    "        根据输入的时间步索引 t，取出对应的时间嵌入向量，并调整形状以匹配特征图 x。\n",
    "        \"\"\"\n",
    "        # 从预计算的 embeddings 表中取出 t 对应的嵌入向量\n",
    "        embeds = self.embeddings[t].to(x.device)\n",
    "\n",
    "        # 添加两个维度 (H=1, W=1)，变为 (B, embed_dim, 1, 1)\n",
    "        # 这样在后续与特征图 x (B, C, H, W) 相加时，embeds 会自动广播到每个空间位置\n",
    "        return embeds[:, :, None, None]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#残差网络， 带时间嵌入注入的残差块，使网络能根据当前时间步 t 调整去噪行为。\n",
    "class ResBlock(nn.Module):\n",
    "    def __init__(self, C: int, num_groups: int, dropout_prob: float):\n",
    "        super().__init__()\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.gnorm1 = nn.GroupNorm(num_groups=num_groups, num_channels=C)\n",
    "        self.gnorm2 = nn.GroupNorm(num_groups=num_groups, num_channels=C)\n",
    "        self.conv1 = nn.Conv2d(C, C, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(C, C, kernel_size=3, padding=1)\n",
    "        self.dropout = nn.Dropout(p=dropout_prob, inplace=True)\n",
    "\n",
    "    def forward(self, x, embeddings):\n",
    "        x = x + embeddings[:, :x.shape[1], :, :]\n",
    "        r = self.conv1(self.relu(self.gnorm1(x)))\n",
    "        r = self.dropout(r)\n",
    "        r = self.conv2(self.relu(self.gnorm2(r)))\n",
    "        return r + x\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#将输入特征图的每个空间位置视为一个 token，计算它们之间的全局依赖关系。\n",
    "class Attention(nn.Module):\n",
    "    def __init__(self, C: int, num_heads: int, dropout_prob: float):\n",
    "        super().__init__()\n",
    "        self.proj1 = nn.Linear(C, C * 3)\n",
    "        self.proj2 = nn.Linear(C, C)\n",
    "        self.num_heads = num_heads\n",
    "        self.dropout_prob = dropout_prob\n",
    "\n",
    "    def forward(self, x):\n",
    "        h, w = x.shape[2:]\n",
    "        x = rearrange(x, 'b c h w -> b (h w) c')#将空间维度展平为序列 \n",
    "        x = self.proj1(x)#线性投影得到 Q, K, V \n",
    "        head_dim = x.shape[-1] // (3 * self.num_heads)# # 计算每个注意力头的维度\n",
    "        \n",
    "        x = rearrange(x, 'b L (K H C) -> K b H L C', K=3, H=self.num_heads, C=head_dim)\n",
    "        q, k, v = x[0], x[1], x[2]\n",
    "        x = F.scaled_dot_product_attention(q, k, v, is_causal=False, dropout_p=self.dropout_prob)\n",
    "        x = rearrange(x, 'b H L C -> b L (H C)')\n",
    "        x = rearrange(x, 'b (h w) C -> b h w C', h=h, w=w)\n",
    "        x = self.proj2(x)\n",
    "        return rearrange(x, 'b h w C -> b C h w')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
